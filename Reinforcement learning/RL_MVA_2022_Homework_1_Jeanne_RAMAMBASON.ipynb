{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAE8kMxe6E6k"
      },
      "source": [
        "# MVA - Homework 1 - Reinforcement Learning (2022/2023)\n",
        "\n",
        "**Name:** RAMAMBASON Jeanne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY4MH0nU637o"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "* The deadline is **November 10 at 11:59 pm (Paris time).**\n",
        "\n",
        "* By doing this homework you agree to the late day policy, collaboration and misconduct rules reported on [Piazza](https://piazza.com/class/l4y5ubadwj64mb/post/6).\n",
        "\n",
        "* **Mysterious or unsupported answers will not receive full credit**. A correct answer, unsupported by calculations, explanation, or algebraic work will receive no credit; an incorrect answer supported by substantially correct calculations and explanations might still receive partial credit.\n",
        "\n",
        "* Answers should be provided in **English**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB__2uUC5U1r"
      },
      "source": [
        "# Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XNj1_VZ2FGJ"
      },
      "source": [
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # install rlberry library\n",
        "  !pip install git+https://github.com/rlberry-py/rlberry.git@mva2021#egg=rlberry[default] > /dev/null 2>&1\n",
        "\n",
        "  # install ffmpeg-python for saving videos\n",
        "  !pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "  # packages required to show video\n",
        "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "  print(\"Libraries installed, please restart the runtime!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8F7RiPXjutB"
      },
      "source": [
        "# Create directory for saving videos\n",
        "!mkdir videos > /dev/null 2>&1\n",
        "\n",
        "# Initialize display and import function to show videos\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KISV44N_nCNm"
      },
      "source": [
        "# Useful libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4hKBRTCh6Gk"
      },
      "source": [
        "# Preparation\n",
        "\n",
        "In the coding exercises, you will use a *grid-world* MDP, which is represented in Python using the interface provided by the [Gym](https://gym.openai.com/) library. The cells below show how to interact with this MDP and how to visualize it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "514mHDeQooKa"
      },
      "source": [
        "from rlberry.envs import GridWorld\n",
        "\n",
        "def get_env():\n",
        "  \"\"\"Creates an instance of a grid-world MDP.\"\"\"\n",
        "  env = GridWorld(\n",
        "      nrows=5,\n",
        "      ncols=7,\n",
        "      reward_at = {(0, 6):1.0},\n",
        "      walls=((0, 4), (1, 4), (2, 4), (3, 4)),\n",
        "      success_probability=0.9,\n",
        "      terminal_states=((0, 6),)\n",
        "  )\n",
        "  return env\n",
        "\n",
        "def render_policy(env, policy=None, horizon=50):\n",
        "  \"\"\"Visualize a policy in an environment\n",
        "\n",
        "  Args:\n",
        "    env: GridWorld\n",
        "        environment where to run the policy\n",
        "    policy: np.array\n",
        "        matrix mapping states to action (Ns).\n",
        "        If None, runs random policy.\n",
        "    horizon: int\n",
        "        maximum number of timesteps in the environment.\n",
        "  \"\"\"\n",
        "  env.enable_rendering()\n",
        "  state = env.reset()                       # get initial state\n",
        "  for timestep in range(horizon):\n",
        "      if policy is None:\n",
        "        action = env.action_space.sample()  # take random actions\n",
        "      else:\n",
        "        action = policy[state]\n",
        "      next_state, reward, is_terminal, info = env.step(action)\n",
        "      state = next_state\n",
        "      if is_terminal:\n",
        "        break\n",
        "  # save video and clear buffer\n",
        "  env.save_video('./videos/gw.mp4', framerate=5)\n",
        "  env.clear_render_buffer()\n",
        "  env.disable_rendering()\n",
        "  # show video\n",
        "  show_video('./videos/gw.mp4')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQAHUBw_ifMI"
      },
      "source": [
        "# Create an environment and visualize it\n",
        "env = get_env()\n",
        "render_policy(env)  # visualize random policy\n",
        "\n",
        "# The reward function and transition probabilities can be accessed through\n",
        "# the R and P attributes:\n",
        "print(f\"Shape of the reward array = (S, A) = {env.R.shape}\")\n",
        "print(f\"Shape of the transition array = (S, A, S) = {env.P.shape}\")\n",
        "print(f\"Reward at (s, a) = (1, 0): {env.R[1, 0]}\")\n",
        "print(f\"Prob[s\\'=2 | s=1, a=0]: {env.P[1, 0, 2]}\")\n",
        "print(f\"Number of states and actions: {env.Ns}, {env.Na}\")\n",
        "\n",
        "# The states in the griworld correspond to (row, col) coordinates.\n",
        "# The environment provides a mapping between (row, col) and the index of\n",
        "# each state:\n",
        "print(f\"Index of state (1, 0): {env.coord2index[(1, 0)]}\")\n",
        "print(f\"Coordinates of state 5: {env.index2coord[5]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibGD_3I89CNu"
      },
      "source": [
        "# Part 1 - Dynamic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR7h5won9NQY"
      },
      "source": [
        "## Question 1.1\n",
        "\n",
        "Consider a general MDP with a discount factor of $\\gamma < 1$. Assume that the horizon is infinite (so there is no termination). A policy $\\pi$ in this MDP\n",
        "induces a value function $V^\\pi$. Suppose an affine transformation is applied to the reward, what is\n",
        "the new value function? Is the optimal policy preserved?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313W4K3B_LtN"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "Let $T \\colon x \\mapsto ax + b$  with $a,b \\in \\mathbb{R}$, the affine transformation. \n",
        "\n",
        "We apply the transformation $T$ to the reward $r$ thus the state value function $V^{\\pi(s),T}$ for an infinite horizon problem with a discount factor  $\\gamma$ < 1 and reward $ar +b$ is: \n",
        "\n",
        "\\begin{aligned}\n",
        "V^{\\pi,T}(s) = & \\mathbb{E}[\\sum_{t=0}^{âˆž}\\gamma^t (ar(s_t, a_t)+b)| s_0=s, a_t~~\\pi(s_t)] \\\\\n",
        "& = a~\\mathbb{E}[\\sum_{t=0}^{âˆž}\\gamma^t r(s_t, a_t)| s_0=s, a_t~~\\pi(s_t)] + \\frac{b}{1-\\gamma}\\\\\n",
        "& = a~V^\\pi(s) + \\frac{b}{1-\\gamma}\n",
        "\\end{aligned}\n",
        "with : $ V^\\pi(s) = \\mathbb{E}[\\sum_{t=0}^{âˆž}\\gamma^t r(s_t, a_t)| s_0=s, a_t~~\\pi(s_t)]  $\n",
        "\n",
        "\n",
        "If  $a>0$ :  $V^{\\pi,T}$ is maximal if $V^{\\pi}$ is maximal so the optimal policy is preserved.\n",
        "\n",
        "\n",
        "If  $a\\leq0$ : the optimal policy isn't preserved because $V^{\\pi,T}$ isn't maximal when $V^{\\pi}$ is maximal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uCVgkDo9vTM"
      },
      "source": [
        "## Question 1.2\n",
        "\n",
        "Consider an infinite-horizon $\\gamma$-discounted MDP. We denote by $Q^*$ the $Q$-function of the optimal policy $\\pi^*$. Prove that, for any function $Q(s, a)$ (which is **not** necessarily the value function of a policy), the following inequality holds for any state $s$:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}||Q^*-Q||_\\infty,\n",
        "$$\n",
        "\n",
        "where $||Q^*-Q||_\\infty = \\max_{s, a} |Q^*(s, a) - Q(s, a)|$ and $\\pi_Q(s) \\in \\arg\\max_a Q(s, a)$. Can you use this result to show that any policy $\\pi$ such that $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$ is optimal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqGWPPD_OAI"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "Let $s \\in S$, a state, so $ Q(s, \\pi_Q(s)) $ and $Q^*(s, \\pi_Q(s))$ are equal to : \n",
        "\n",
        "$$\n",
        "Q(s, \\pi_Q(s)) = r(s, \\pi_Q(s))+\\gamma \\sum_s'p(s'|s, \\pi_Q(s))V^{\\pi_Q}(s)=T^{\\pi_Q}V^{\\pi_Q}(s)\\\\\n",
        "Q^*(s, \\pi_Q(s)) = r(s, \\pi_Q(s))+\\gamma \\sum_s'p(s'|s, \\pi_Q(s))V^*(s)=T^{\\pi_Q}V^*(s)\n",
        "$$\n",
        "\n",
        "Moreover :  \n",
        "\n",
        "\\begin{aligned}\n",
        "V^*(s)-V^{\\pi_Q}(s)\n",
        "& = Q^*(s, \\pi^*(s)) - Q(s, \\pi_Q(s))\\\\\n",
        "& = \\left(Q^*(s, \\pi^*(s)) - Q(s, \\pi^*(s))\\right) + \\left(Q(s, \\pi^*(s)) - Q^*(s, \\pi_Q(s))\\right) +  \\left(Q^*(s, \\pi_Q(s)) - Q(s, \\pi_Q(s))\\right) \\\\\n",
        "\\end{aligned}\n",
        "By definition, $Q(s, \\pi^*(s)) \\leq Q(s, \\pi_Q(s))$, so:\n",
        "\\begin{aligned}\n",
        "|V^*(s)-V^{\\pi_Q}(s)|\n",
        "& \\leq ||Q^*-Q||_âˆž + |Q(s, \\pi_Q(s)) - Q^*(s, \\pi_Q(s))|+  |Q^*(s, \\pi_Q(s)) - Q(s, \\pi_Q(s))| \\\\\n",
        "& \\leq 2||Q^*-Q||_âˆž + |T^{\\pi_Q}V^*(s) - T^{\\pi_Q}V^{\\pi_Q}(s)|\n",
        "\\end{aligned}\n",
        "\n",
        "Thus:\n",
        "\n",
        "\\begin{aligned}\n",
        "||V^*-V^{\\pi_Q}||_âˆž & \\leq 2||Q^*-Q||_âˆž + ||T^{\\pi_Q}V^* - T^{\\pi_Q}V^{\\pi_Q}||_âˆž \\\\\n",
        "& \\leq 2||Q^*-Q||_âˆž + \\gamma||V^* - V^{\\pi_Q}||_âˆž \n",
        "\\end{aligned}\\\\\n",
        "\n",
        "\n",
        "We got : $(1-\\gamma) ||V^*-V^{\\pi_Q}||_âˆž \\leq 2||Q^*-Q||_âˆž$, so for all $s \\in S$, we have finally: \n",
        "$$\n",
        "V^{\\pi_Q}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}||Q^*-Q||_\\infty.\n",
        "$$\n",
        "And if $Q=Q^*$ : \n",
        "$$\n",
        "V^{\\pi_{Q^*}}(s) \\geq V^*(s)$$\n",
        "Thus, any policy $\\pi$ such as $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$ is optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIrtb7sihYcM"
      },
      "source": [
        "## Question 1.3\n",
        "\n",
        "In this question, you will implement and compare the policy and value iteration algorithms for a finite MDP. \n",
        "\n",
        "Complete the functions `policy_evaluation`, `policy_iteration` and `value_iteration` below.\n",
        "\n",
        "\n",
        "Compare value iteration and policy iteration. Highlight pros and cons of each method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLmQtk-wt0HS"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "[pros/cons of each method + implementation below]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "A_P2vvRcuknM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yI0YYtMmpDQ"
      },
      "source": [
        "def policy_evaluation(P, R, policy, gamma=0.9, tol=1e-2):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        policy: np.array\n",
        "            matrix mapping states to action (Ns)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        value_function: np.array\n",
        "            The value function of the given policy\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    value_function = np.zeros(Ns)\n",
        "    # ====================================================\n",
        "\t  # YOUR IMPLEMENTATION HERE \n",
        "    #\n",
        "    delta = tol + 1\n",
        "\n",
        "    while delta > tol:\n",
        " \n",
        "      \n",
        "      delta = 0 \n",
        "      for s in range(Ns):\n",
        "        \n",
        "        a = policy[s] \n",
        "        V = R[s,a]\n",
        "\n",
        "        for s_p in range(Ns):\n",
        "          V +=  gamma*P[s,a,s_p]*value_function[s_p]\n",
        "        \n",
        "        delta = max(delta, np.abs(V - value_function[s]))\n",
        "        value_function[s] = V\n",
        "      \n",
        "    # ====================================================\n",
        "    return value_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncqbPx99ncVY"
      },
      "source": [
        "def policy_iteration(P, R, gamma=0.9, tol=1e-3):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        policy: np.array\n",
        "            the final policy\n",
        "        V: np.array\n",
        "            the value function associated to the final policy\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    V_old = np.zeros(Ns)\n",
        "    policy = np.ones(Ns, dtype=int)\n",
        "    # ====================================================\n",
        "\t  # YOUR IMPLEMENTATION HERE \n",
        "    #\n",
        "    V = policy_evaluation(P, R, policy, gamma=gamma, tol=tol)\n",
        "    delta = tol + 1\n",
        "\n",
        "    it = 0\n",
        "    list_time_it = []\n",
        "\n",
        "    while delta > tol:\n",
        "      it+= 1\n",
        "      ts = time.time()\n",
        "\n",
        "      delta = 0\n",
        "      policy_old = np.copy(policy)\n",
        "      V_old = np.copy(V)\n",
        "\n",
        "      for s in range(Ns):\n",
        "        policy[s] = np.argmax(R[s,:] + gamma*P[s,:,:].dot(V))\n",
        "      \n",
        "      V = policy_evaluation(P, R, policy, gamma=gamma, tol=tol)\n",
        "      \n",
        "      tf = time.time() - ts\n",
        "      list_time_it.append(tf)\n",
        "      \n",
        "      delta = np.max( np.abs(V - V_old))\n",
        "      #delta = np.linalg.norm(V - V_old)\n",
        "    # ====================================================\n",
        "    return policy, V, it, list_time_it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jB7dfA5nRCZ"
      },
      "source": [
        "def value_iteration(P, R, gamma=0.9, tol=1e-3):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        Q: final Q-function (at iteration n)\n",
        "        greedy_policy: greedy policy wrt Qn\n",
        "        Qfs: all Q-functions generated by the algorithm (for visualization)\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    Q_old = np.zeros((Ns, Na))\n",
        "    Q = np.zeros((Ns, Na))\n",
        "    Qfs = [Q]\n",
        "    # ====================================================\n",
        "\t  # YOUR IMPLEMENTATION HERE \n",
        "    #\n",
        "\n",
        "    delta = tol + 1\n",
        "    it = 0\n",
        "    list_time_it = []\n",
        "    while delta > tol : \n",
        "      it+= 1\n",
        "      ts = time.time()\n",
        "\n",
        "      Q_old = Q\n",
        "      Q = R + gamma * P.dot( np.max(Q_old, axis=1))\n",
        "      greedy_policy = np.argmax(Q, axis=1)\n",
        "      Qfs.append(np.copy(Q))\n",
        "\n",
        "      delta = np.max(np.abs(Q - Q_old))\n",
        "      #delta = np.linalg.norm(Q - Q_old)\n",
        "\n",
        "      tf = time.time() - ts\n",
        "      list_time_it.append(tf)\n",
        "\n",
        "    greedy_policy = np.argmax(Q, axis=1)\n",
        "    # ====================================================\n",
        "    return Q, greedy_policy, Qfs, it , list_time_it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fi0IzZJp74Z"
      },
      "source": [
        "### Testing your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7JKrc1oqFI2"
      },
      "source": [
        "# Parameters\n",
        "tol = 1e-5\n",
        "gamma = 0.99\n",
        "\n",
        "# Environment\n",
        "env = get_env()\n",
        "\n",
        "# run value iteration to obtain Q-values\n",
        "VI_Q, VI_greedypol, all_qfunctions, VI_nb_it, VI_list_time_it = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
        "\n",
        "# render the policy\n",
        "print(\"[VI]Greedy policy: \")\n",
        "render_policy(env, VI_greedypol)\n",
        "\n",
        "# compute the value function of the greedy policy using matrix inversion\n",
        "# ====================================================\n",
        "# YOUR IMPLEMENTATION HERE \n",
        "# compute value function of the greedy policy\n",
        "#\n",
        "greedy_V = policy_evaluation(env.P,env.R, VI_greedypol, gamma= gamma, tol= tol )\n",
        "\n",
        "# ====================================================\n",
        "\n",
        "# show the error between the computed V-functions and the final V-function\n",
        "# (that should be the optimal one, if correctly implemented)\n",
        "# as a function of time\n",
        "final_V = all_qfunctions[-1].max(axis=1)\n",
        "norms = [ np.linalg.norm(q.max(axis=1) - final_V) for q in all_qfunctions]\n",
        "plt.plot(norms)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Value iteration: convergence\")\n",
        "\n",
        "#### POLICY ITERATION ####\n",
        "PI_policy, PI_V, PI_nb_it, PI_list_time_it = policy_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
        "print(\"\\n[PI]final policy: \")\n",
        "render_policy(env, PI_policy)\n",
        "\n",
        "## Uncomment below to check that everything is correct\n",
        "assert np.allclose(PI_policy, VI_greedypol),\\\n",
        "     \"You should check the code, the greedy policy computed by VI is not equal to the solution of PI\"\n",
        "assert np.allclose(PI_V, greedy_V),\\\n",
        "    \"Since the policies are equal, even the value function should be\"\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"###### for Value Iteration Method ######\")\n",
        "print(\"Number of iterations :\", VI_nb_it,\"; Average time for each iteration :\", np.mean(np.array(VI_list_time_it)), \"s ; Total Compute time : \", np.sum(np.array(VI_list_time_it)),\"s\")\n",
        "print(\"\\n\")\n",
        "print(\"###### for Policy Iteration Method ######\")\n",
        "print(\"Number of iterations :\", PI_nb_it,\"; Average time for each iteration :\", np.mean(np.array(PI_list_time_it)), \"s ; Total Compute time : \", np.sum(np.array(PI_list_time_it)),\"s\")\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "OyQ8IKO8vq4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Response :** \n",
        "In theory : PI must perform policy evaluation on each iteration which involves solving a linear system, thus PI is slower than VI for each iteration. VI is easier to implement since it does not require the policy evaluation step. Both PI and VI are guaranted to converge but PI requires a lot less iteration to converge.\n",
        "\n",
        "In practice : Policy Iteration (PI) requires a lot fewer steps than Value Iteration (VI) $(4<< 1147)$ but the computation time of each iteration is a lot quicker for Value Iteration than for Policy Iteraiton : 3e-5 s<< 0.6 s. PI needs less iteration beacause at each iteration, it evaluate and improve the policy whereas VI runs through all possible actions at once to find the maximum action value. But PI iteration are slower because it computes the value function at each steps, ie resolve a linear system."
      ],
      "metadata": {
        "id": "e8vwzG2Zwpjd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V1QdoH-xFX0"
      },
      "source": [
        "# Part 2 - Tabular RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf51VhoPxbV4"
      },
      "source": [
        "## Question 2.1\n",
        "\n",
        "The code below collects two datasets of transitions (containing states, actions, rewards and next states) for a discrete MDP.\n",
        "\n",
        "For each of the datasets:\n",
        "\n",
        "1. Estimate the transitions and rewards, $\\hat{P}$ and $\\hat{R}$.\n",
        "2. Compute the optimal value function and the optimal policy with respect to the estimated MDP (defined by $\\hat{P}$ and $\\hat{R}$), which we denote by $\\hat{\\pi}$ and $\\hat{V}$.\n",
        "3. Numerically compare the performance of $\\hat{\\pi}$ and $\\pi^\\star$ (the true optimal policy), and the error between $\\hat{V}$ and $V^*$ (the true optimal value function).\n",
        "\n",
        "Which of the two data collection methods do you think is better? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWSyewG2EZpJ"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "[answer last question + implementation below]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lNPhB28EcGd"
      },
      "source": [
        "def get_random_policy_dataset(env, n_samples):\n",
        "  \"\"\"Get a dataset following a random policy to collect data.\"\"\"\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "  \n",
        "  state = env.reset()\n",
        "  for _ in range(n_samples):\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, is_terminal, info = env.step(action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "    # update state\n",
        "    state = next_state\n",
        "    if is_terminal:\n",
        "      state = env.reset()\n",
        "\n",
        "  dataset = (states, actions, rewards, next_states)\n",
        "  return dataset\n",
        "\n",
        "def get_uniform_dataset(env, n_samples):\n",
        "  \"\"\"Get a dataset by uniformly sampling states and actions.\"\"\"\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "  for _ in range(n_samples):\n",
        "    state = env.observation_space.sample()\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, is_terminal, info = env.sample(state, action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "\n",
        "  dataset = (states, actions, rewards, next_states)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Collect two different datasets\n",
        "num_samples = 500\n",
        "env = get_env()\n",
        "dataset_1 = get_random_policy_dataset(env, num_samples)\n",
        "dataset_2 = get_uniform_dataset(env, num_samples)\n",
        "\n",
        "\n",
        "# Item 3: Estimate the MDP with the two datasets; compare the optimal value\n",
        "# functions in the true and in the estimated MDPs\n",
        "\n",
        "# ...\n",
        "Ns, Na = env.R.shape\n",
        "\n",
        "def estimate_P_and_R(dataset,Na, Ns, num_samples):\n",
        "  (states, actions, rewards, next_states) = dataset\n",
        "\n",
        "  P_est = np.zeros(((Ns,Na,Ns)))\n",
        "  R_est = np.zeros((Ns,Na))\n",
        "\n",
        "\n",
        "  for i in range(len(states)):\n",
        "    P_est[states[i],actions[i],next_states[i]] += 1\n",
        "  \n",
        "  for i in range(len(rewards)):\n",
        "        R_est[states[i],actions[i]] += rewards[i]\n",
        "\n",
        "  for s in range(Ns):\n",
        "    for a in range(Na):\n",
        "      if P_est[s,a,:].sum()>1e-3:\n",
        "        temp = P_est[s,a,:].sum()\n",
        "        R_est[s,a] /= temp\n",
        "        P_est[s,a,:] = P_est[s,a,:] / temp\n",
        "        \n",
        "\n",
        "  return P_est,R_est"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_est_1,R_est_1 = estimate_P_and_R(dataset_1,Na, Ns, num_samples)\n",
        "\n",
        "P_est_2,R_est_2 = estimate_P_and_R(dataset_2,Na, Ns,num_samples)"
      ],
      "metadata": {
        "id": "16tYKfe83R91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "tol = 1e-3\n",
        "gamma = 0.9\n"
      ],
      "metadata": {
        "id": "0-1SRbQcmy8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PI_policy_true, PI_V_true, _, _ = policy_iteration(env.P,env.R, gamma=gamma, tol=tol)\n",
        "\n",
        "PI_policy_est_1, PI_V_est_1, _, _ = policy_iteration(P_est_1,R_est_1, gamma=gamma, tol=tol)\n",
        "\n",
        "PI_policy_est_2, PI_V_est_2, _, _ = policy_iteration(P_est_2,R_est_2, gamma=gamma, tol=tol)"
      ],
      "metadata": {
        "id": "R0tmqGBOh2Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "YymyQpRVuuIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('MAE for dataset 1', mean_absolute_error(PI_V_true, PI_V_est_1))\n",
        "print('MAE for dataset 2', mean_absolute_error(PI_V_true, PI_V_est_2))\n",
        "print('\\n')\n",
        "print('MSE for dataset 1',mean_squared_error(PI_V_true, PI_V_est_1))\n",
        "print('MSE for dataset 2',mean_squared_error(PI_V_true, PI_V_est_2))"
      ],
      "metadata": {
        "id": "DDbDnTo1ux6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Optimal policy\")\n",
        "render_policy(env, PI_policy_true)\n",
        "print(\"Optimal estimated policy for dataset 1\")\n",
        "render_policy(env, PI_policy_est_1)\n",
        "print(\"Optimal estimated policy for dataset 2\")\n",
        "render_policy(env, PI_policy_est_2)\n"
      ],
      "metadata": {
        "id": "lAQ3p03GhcrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Response :** \n",
        "The second data collection method : the uniformly sampling, is better because at each step, it chooses randomly a state and an action whereas the random policy sampling starts from the initial state and then only chooses the action randomly. Thus, the uniformly sampling allows more exploration. \n",
        "\n",
        "In pratice, we see that the mean absolute error and mean squared error of the value fonction estimated with dataset 2 are much better than with dataset 1.\n"
      ],
      "metadata": {
        "id": "LyX502_JyC9a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKINsa_yGLGL"
      },
      "source": [
        "## Question 2.2\n",
        "\n",
        "Suppose that $\\hat{P}$ and $\\hat{R}$ are estimated from a dataset of exactly $N$ i.i.d. samples from **each** state-action pair. This means that, for each $(s,a)$, we have $N$ samples $\\{(s_1',r_1, \\dots, s_N', r_N\\}$, where $s_i' \\sim P(\\cdot | s,a)$ and $r_i \\sim R(s,a)$ for $i=1,\\dots,N$, and\n",
        "$$ \\hat{P}(s'|s,a) = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}(s_i' = s'), $$\n",
        "$$ \\hat{R}(s,a) = \\frac{1}{N}\\sum_{i=1}^N r_i.$$\n",
        "Suppose that $R$ is a distribution with support in $[0,1]$. Let $\\hat{V}$ be the optimal value function computed in the empirical MDP (i.e., the one with transitions $\\hat{P}$ and rewards $\\hat{R}$). For any $\\delta\\in(0,1)$, derive an upper bound to the error\n",
        "\n",
        "$$ \\| \\hat{V} - V^* \\|_\\infty $$\n",
        "\n",
        "which holds with probability at least $1-\\delta$.\n",
        "\n",
        "**Note** Your bound should only depend on deterministic quantities like $N$, $\\gamma$, $\\delta$, $S$, $A$. It should *not* dependent on the actual random samples.\n",
        "\n",
        "**Hint** The following two inequalities may be helpful.\n",
        "\n",
        "1. **A (simplified) lemma**. For any state $\\bar{s}$,\n",
        "\n",
        "$$ |\\hat{V}(\\bar{s}) - V^*(\\bar{s})| \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$$\n",
        "\n",
        "2. **Hoeffding's inequality**. Let $X_1, \\dots X_N$ be $N$ i.i.d. random variables bounded in the interval $[0,b]$ for some $b>0$. Let $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i$ be the empirical mean. Then, for any $\\epsilon > 0$,\n",
        "\n",
        "$$ \\mathbb{P}(|\\bar{X} - \\mathbb{E}[\\bar{X}]| > \\epsilon) \\leq 2e^{-\\frac{2N\\epsilon^2}{b^2}}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKmdulLaMoiN"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "Let's note $X_i = r_i + \\gamma \\sum_{s'} \\mathbb{1}_{s_i' = s'} V^*(s')$ for all $i \\in  {1,..,N} $. $(X_i)_{(1\\leq i \\leq N)}$ are $ð‘ $ iid random variables because $s'_i$ and $r_i$ are idd variables. \n",
        "\n",
        "Moreover, let's note $\\bar{X}_{s,a} = \\frac{1}{N}\\sum_{i=1}^N X_i $. \n",
        "\n",
        "Thus, $\\bar{X}_{s,a} =  \\frac{1}{N}\\sum\\limits_{i=1}^N r_i + \\gamma \\sum\\limits_{s'}\\frac{1}{N}\\sum\\limits_{i=1}^N \\mathbb{1}_{s_i' = s'}V^*(s') = \\hat{R}(s,a) + \\gamma\\sum\\limits_{s'} \\hat{P}(s'\\mid s,a) V^*(s') $ and $\\mathbb{E}[\\bar{X}_{s,a}] = R(s,a) + \\gamma\\sum\\limits_{s'} P(s'\\mid s,a) V^*(s')$.\n",
        "\n",
        "We got :  \n",
        "$ \\mid \\bar{X}_{s,a} - \\mathbb{E}[\\bar{X}_{s,a}] \\mid  = \\mid R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s')  \\mid$\n",
        "\n",
        "Thanks to the first lemma, we got for any state $\\bar{s}$:  \n",
        "$ \\mid \\hat{V}(\\bar{s}) - V^*(\\bar{s}) \\mid \\leq \\frac{1}{1-\\gamma} \\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$\n",
        "\n",
        "\n",
        "Thus,\n",
        "$\\max_{\\bar{s}} |\\hat{V}(\\bar{s}) - V^*(\\bar{s})|  =  \\| \\hat{V} - V^* \\|_\\infty \\leq \\frac{1}{1-\\gamma} \\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| \n",
        "$\n",
        "ie :  $\\| \\hat{V} - V^* \\|_\\infty \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\mid \\bar{X}_{s,a} - \\mathbb{E}[\\bar{X}_{s,a}] \\mid  $\n",
        "\n",
        "\n",
        "So for $\\epsilon>0$, \n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbb{P} \\left( ||\\hat{V} - V^*||_{\\infty} \\leq \\epsilon \\right) \n",
        "& \\geq \\mathbb{P} \\left( \\frac{1}{1-\\gamma}\\max_{s,a} \\mid \\bar{X}_{s,a} - \\mathbb{E}[\\bar{X}_{s,a}] \\mid \\leq \\epsilon \\right)\\\\\n",
        "& = \\prod\\limits_{s,a} \\mathbb{P} \\left( \\frac{1}{1-\\gamma} \\mid \\bar{X}_{s,a} - \\mathbb{E}[\\bar{X}_{s,a}] \\mid \\leq \\epsilon \\right) ~~~~~~~~\\textrm{â† because samples are independant}\\\\\n",
        "& = \\prod\\limits_{s,a} \\mathbb{P} \\left( \\mid \\bar{X}_{s,a} - \\mathbb{E}[\\bar{X}_{s,a}] \\mid \\leq \\epsilon (1-\\gamma) \\right)\\\\\n",
        "\\end{aligned}\n",
        "\n",
        "Moreover, $(X_i)_{(1\\leq i \\leq N)}$ are bounded in the interval $[0, \\frac{1}{1-\\gamma} ]$: \n",
        "\\begin{aligned}\n",
        "\\mid X_i \\mid & \\leq \\mid r_i| +  \\gamma \\sum\\limits_{s'} \\mathbb{1}_{s_i' = s'}|V^*(s)\\mid \\\\\n",
        "&\\leq 1 +  \\gamma \\sum\\limits_{s'}|V^*(s')| \\textrm{â†  because $R$  is a distribution with support in [0,1]} \\\\\n",
        "&\\leq 1 +  \\gamma \\sum\\limits_{s'} \\mathbb{E}\\left[\\sum\\limits_{t=0}^{\\infty} \\gamma^t |R(s_t,a_t)| \\mid s_0 = s'\\right] \\\\\n",
        "&\\leq 1 +  \\gamma \\sum\\limits_{t=0}^{\\infty} \\gamma^t\\\\\n",
        "& = 1 + \\frac{\\gamma}{1-\\gamma} \\\\\n",
        "& =\\frac{1}{1-\\gamma}\n",
        "\\end{aligned}\n",
        "\n",
        "So, we can apply the Hoeffding's inequality on all $\\bar{X}_{s,a}$ with $b = \\frac{1}{1-\\gamma}$: \n",
        "\\begin{aligned}\n",
        "\\mathbb{P} \\left( ||\\hat{V} - V^*||_{\\infty} \\leq \\epsilon \\right) \n",
        "& \\geq \\prod\\limits_{s,a} \\mathbb{P} \\left( \\mid \\bar{X}_{s,a} - \\mathbb{E}[\\bar{X}_{s,a}] \\mid \\leq \\epsilon (1-\\gamma) \\right)\\\\\n",
        "& \\geq (1 - 2e^{-2N \\epsilon^2(1-\\gamma)^4})^{N_s N_a}\n",
        "\\end{aligned}\n",
        "with $N_s = |S|$ and $N_a = |A|$.\n",
        "\n",
        "\n",
        "We choose $\\epsilon$ such as $(1 - 2e^{-2N \\epsilon^2(1-\\gamma)^4})^{N_s N_a} = 1-\\delta$. \n",
        "\n",
        "\n",
        "Thus with $\\epsilon = \\sqrt{- \\frac{\\ln \\left(\\frac{1 - (1 - \\delta)^{\\frac{1}{N_s N_a}}}{2}\\right)}{2(1-\\gamma)^4 N}}$, we got $\\mathbb{P} \\left( ||\\hat{V} - V^*||_{\\infty} \\leq \\epsilon \\right) \\geq 1 -\\delta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpqwCBG2MwxO"
      },
      "source": [
        "## Question 2.3\n",
        "\n",
        "Suppose once again that we are given a dataset of $N$ samples in the form of tuples $(s_i,a_i,s_i',r_i)$. We know that each tuple contains a valid transition from the true MDP, i.e., $s_i' \\sim P(\\cdot | s_i, a_i)$ and $r_i \\sim R(s_i,a_i)$, while the state-action pairs $(s_i,a_i)$ from which the transition started can be arbitrary.\n",
        "\n",
        "Suppose we want to apply Q-learning to this MDP. Can you think of a way to leverage this offline data to improve the sample-efficiency of the algorithm? What if we were using SARSA instead?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbYTKetHOYU_"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "$\\rightarrow$ Q learning is an off-policy learning algorithm, ie it can use any behavioral policy. \n",
        "To improve the sampling efficiency, we use the implementation of the Tabular Dyna-Q algorithm, the exploration policy used is the $\\epsilon$-greedy policy derived from $Q$ estimated, so that we are closer to the real policy. \n",
        "\n",
        "\n",
        "$\\rightarrow$ SARSA is an on-policy learning algorithm, ie it uses the value candidate $\\hat{Q}$ to define the policy used to select the action. To improve the sampling efficiency of the algorithm, we can use an Importance sampling method which uses a weighted distribution. This weighted distribution favours \"important\" samples ie samples close to the target distribution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "542QxKsSOs21"
      },
      "source": [
        "# Part 3 - RL with Function Approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiGZBiJ4PiIE"
      },
      "source": [
        "## Question 3.1\n",
        "\n",
        "Given a datset $(s_i, a_i, r_i, s_i')$ of (states, actions, rewards, next states), the Fitted Q-Iteration (FQI) algorithm proceeds as follows:\n",
        "\n",
        "\n",
        "* We start from a $Q$ function $Q_0 \\in \\mathcal{F}$, where $\\mathcal{F}$ is a function space;\n",
        "* At every iteration $k$, we compute $Q_{k+1}$ as:\n",
        "\n",
        "$$\n",
        "Q_{k+1}\\in\\arg\\min_{f\\in\\mathcal{F}} \\frac{1}{2}\\sum_{i=1}^N\n",
        "\\left(\n",
        "  f(s_i, a_i) - y_i^k\n",
        "\\right)^2 + \\lambda \\Omega(f)\n",
        "$$\n",
        "where $y_i^k = r_i + \\gamma \\max_{a'}Q_k(s_i', a')$, $\\Omega(f)$ is a regularization term and $\\lambda > 0$ is the regularization coefficient.\n",
        "\n",
        "\n",
        "Consider FQI with *linear* function approximation. That is, for a given feature map $\\phi : S \\rightarrow \\mathbb{R}^d$, we consider a parametric family of $Q$ functions $Q_\\theta(s,a) = \\phi(s)^T\\theta_a$ for $\\theta_a\\in\\mathbb{R}^d$. Suppose we are applying FQI on a given dataset of $N$ tuples of the form $(s_i, a_i, r_i, s_i')$ and we are at the $k$-th iteration. Let $\\theta_k \\in\\mathbb{R}^{d \\times A}$ be our current parameter. Derive the *closed-form* update to find $\\theta_{k+1}$, using $\\frac{1}{2}\\sum_a ||\\theta_a||_2^2$ as regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jx7aE41DkEM"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "At each iteration $k$, we compute $Q_{k+1}$ as: \n",
        "$$\n",
        "Q_{k+1}\\in\\arg\\min_{\\theta\\in\\mathbb{R}^{d \\times A}} \\frac{1}{2}\\sum_{i=1}^N\n",
        "\\left(\n",
        "  \\phi(s)^T\\theta_{a_i} - y_i^k\n",
        "\\right)^2 + \\frac{\\lambda }{2}\\sum_a ||\\theta_a||_2^2\n",
        "$$\n",
        "\n",
        "ie : $$\n",
        "Q_{k+1}\\in\\arg\\min_{\\theta\\in\\mathbb{R}^{d \\times A}} F(\\theta)\n",
        "$$\n",
        "with \n",
        "\\begin{aligned}\n",
        "F(\\theta) \n",
        "&=  \\frac{1}{2}\\sum_{i=1}^N \\left( \\phi(s)^T\\theta_{a_i} - y_i^k \\right)^2 + \\frac{\\lambda }{2}\\sum_a ||\\theta_a||_2^2 \\\\\n",
        "\\end{aligned}\n",
        "\n",
        "The minimum of $F(\\theta) $ : $\\theta_{k+1}$, satisfies $\\nabla_{\\theta}F(\\theta_{k+1}) = 0$. Let's compute $\\theta_{k+1}$ such as for all $a \\in A$, $\\nabla_{\\theta_a}F(\\theta_{k+1}) = 0$ thus $\\nabla_{\\theta}F(\\theta_{k+1}) = 0$.\n",
        "\n",
        "For $a \\in A$, \n",
        "$$\\nabla_{\\theta_a}F(\\theta_{k+1})  = \\sum\\limits_{\\substack{i=0 \\\\ s.t.~a_i = a}}^N \\left( (\\phi(s_i)^T \\theta_{a_i} - y_i^k )\\phi(s_i) + \\lambda \\theta_{a_i} \\right) = 0 $$\n",
        "$\\iff \\theta_{k+1,a} = \\left(\\sum\\limits_{\\substack{i=0 \\\\ s.t. a_i = a}}^N \\phi(s_i)\\phi(s_i)^T + \\lambda I_d \\right)^{-1} \\sum\\limits_{\\substack{i=0 \\\\ s.t.~a_i = a}}^N y_i^k \\phi(s_i)$\n",
        "\n",
        "And, $\\theta_{k+1} = (\\theta_{k+1,a})_{a \\in A}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewHzjm7MVGBg"
      },
      "source": [
        "## Question 3.2\n",
        "\n",
        "The code below creates a larger gridworld (with more states than the one used in the previous questions), and defines a feature map. Implement linear FQI to this environment (in the function `linear_fqi()` below), and compare the approximated $Q$ function to the optimal $Q$ function computed with value iteration.\n",
        "\n",
        "Can you improve the feature map in order to reduce the approximation error?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu4g-HSnEcBs"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "[explanation about how you tried to reduce the approximation error + FQI implementation below]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZovF3VXOVfCs"
      },
      "source": [
        "def get_large_gridworld():\n",
        "  \"\"\"Creates an instance of a grid-world MDP with more states.\"\"\"\n",
        "  walls = [(ii, 10) for ii in range(15) if (ii != 7 and ii != 8)]\n",
        "  env = GridWorld(\n",
        "      nrows=15,\n",
        "      ncols=15,\n",
        "      reward_at = {(14, 14):1.0},\n",
        "      walls=tuple(walls),\n",
        "      success_probability=0.9,\n",
        "      terminal_states=((14, 14),)\n",
        "  )\n",
        "  return env\n",
        "\n",
        "\n",
        "class GridWorldFeatureMap:\n",
        "  \"\"\"Create features for state-action pairs\n",
        "  \n",
        "  Args:\n",
        "    dim: int\n",
        "      Feature dimension\n",
        "    sigma: float\n",
        "      RBF kernel bandwidth\n",
        "  \"\"\"\n",
        "  def __init__(self, env, dim=15, sigma=0.25):\n",
        "    self.index2coord = env.index2coord\n",
        "    self.n_states = env.Ns\n",
        "    self.n_actions = env.Na\n",
        "    self.dim = dim\n",
        "    self.sigma = sigma\n",
        "\n",
        "    n_rows = env.nrows\n",
        "    n_cols = env.ncols\n",
        "\n",
        "    # build similarity matrix\n",
        "    sim_matrix = np.zeros((self.n_states, self.n_states))\n",
        "    for ii in range(self.n_states):\n",
        "        row_ii, col_ii = self.index2coord[ii]\n",
        "        x_ii = row_ii / n_rows\n",
        "        y_ii = col_ii / n_cols\n",
        "        for jj in range(self.n_states):\n",
        "            row_jj, col_jj = self.index2coord[jj]\n",
        "            x_jj = row_jj / n_rows\n",
        "            y_jj = col_jj / n_cols\n",
        "            dist = np.sqrt((x_jj - x_ii) ** 2.0 + (y_jj - y_ii) ** 2.0)\n",
        "            sim_matrix[ii, jj] = np.exp(-(dist / sigma) ** 2.0)\n",
        "\n",
        "    # factorize similarity matrix to obtain features\n",
        "    uu, ss, vh = np.linalg.svd(sim_matrix, hermitian=True)\n",
        "    self.feats = vh[:dim, :]\n",
        "\n",
        "  def map(self, observation):\n",
        "    feat = self.feats[:, observation].copy()\n",
        "    return feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InCfu7F9-TbS"
      },
      "source": [
        "env = get_large_gridworld()\n",
        "feat_map = GridWorldFeatureMap(env)\n",
        "\n",
        "# Visualize large gridworld\n",
        "render_policy(env)\n",
        "\n",
        "# The features have dimension (feature_dim).\n",
        "feature_example = feat_map.map(1) # feature representation of s=1\n",
        "print(feature_example)\n",
        "\n",
        "# Initial vector theta representing the Q function\n",
        "theta = np.zeros((feat_map.dim, env.action_space.n))\n",
        "print(theta.shape)\n",
        "print(feature_example @ theta) # approximation of Q(s=1, a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_fqi(env, feat_map, num_iterations, lambd=0.1, gamma=0.95):\n",
        "  \"\"\"\n",
        "  # Linear FQI implementation\n",
        "  # TO BE COMPLETED\n",
        "  \"\"\"\n",
        "  \n",
        "  n_samples = 5000\n",
        "  # get a dataset\n",
        "  dataset = get_uniform_dataset(env, n_samples= n_samples )\n",
        "  # dataset = get_random_policy_dataset(env, n_samples= n_samples )\n",
        "\n",
        "  (states, actions, rewards, next_states) = dataset\n",
        "\n",
        "  R_max = max(rewards) \n",
        "\n",
        "  theta_old = np.zeros((feat_map.dim, env.Na))\n",
        "  theta_new = np.zeros((feat_map.dim, env.Na))\n",
        "\n",
        "  for it in range(num_iterations):\n",
        "    if it%10 == 0:\n",
        "      print('it nÂ°',it)\n",
        "\n",
        "    theta_old = np.copy(theta_new)\n",
        "\n",
        "    ## calcul de y_k\n",
        "    y_k = np.zeros(n_samples)\n",
        "    for i in range(n_samples):\n",
        "      Q_k = np.array([feat_map.map(next_states[i]).dot(theta_old[:,a]) for a in range(env.Na)] )\n",
        "      f = max(Q_k)       \n",
        "      #clipping \n",
        "      if f > 0 :\n",
        "        f = min(f , R_max / (1-gamma) )\n",
        "      else : \n",
        "        f = max(f, - R_max/(1-gamma))\n",
        "\n",
        "      y_k[i] = rewards[i] + gamma* f\n",
        "\n",
        "\n",
        "    ## calcul de theta_{k+1} : theta_new\n",
        "    \n",
        "    theta_new = np.zeros((feat_map.dim, env.Na))\n",
        "    for a in range(env.Na):\n",
        "      index_list = [i for i, x in enumerate(actions) if x == a]\n",
        "\n",
        "      if len(index_list) != 0:\n",
        "        mat_to_inv = sum([np.matmul(np.array([feat_map.map(states[i])]).T, np.array([feat_map.map(states[i])])) for i in index_list]) + lambd*np.eye(feat_map.dim)\n",
        "        mat_sum = sum([y_k[i]*feat_map.map(states[i]) for i in index_list])\n",
        "        theta_new[:,a] = np.matmul(np.linalg.inv(mat_to_inv),mat_sum)\n",
        "\n",
        "  return theta_new\n"
      ],
      "metadata": {
        "id": "U06wVSAvSZPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21KMmruugO1"
      },
      "source": [
        "\n",
        "# ----------------------------\n",
        "# Environment and feature map\n",
        "# ----------------------------\n",
        "env = get_large_gridworld()\n",
        "# you can change the parameters of the feature map, and even try other maps!\n",
        "feat_map = GridWorldFeatureMap(env, dim=50, sigma=0.25)\n",
        "\n",
        "# -------\n",
        "# Run FQI\n",
        "# -------\n",
        "theta = linear_fqi(env, feat_map, num_iterations= 100)\n",
        "\n",
        "# Compute and run greedy policy\n",
        "Q_fqi = np.zeros((env.Ns, env.Na))\n",
        "for ss in range(env.Ns):\n",
        "  state_feat = feat_map.map(ss)\n",
        "  Q_fqi[ss, :] = state_feat @ theta\n",
        "\n",
        "V_fqi = Q_fqi.max(axis=1)\n",
        "policy_fqi = Q_fqi.argmax(axis=1)\n",
        "render_policy(env, policy_fqi, horizon=100)\n",
        "\n",
        "# Visualize the approximate value function in the gridworld.\n",
        "img = env.get_layout_img(V_fqi)    \n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Response**\n",
        "\n",
        "\n",
        " To reduce the approximation error:\n",
        " \n",
        "$\\rightarrow$ I clipped $f$ ie when $max Q_k > \\frac{Rmax}{1-\\gamma}$ (resp. $< -\\frac{Rmax}{1-\\gamma}$), I used $\\frac{Rmax}{1-\\gamma}$ (resp. $ -\\frac{Rmax}{1-\\gamma}$) to compute $y^i_k$ \n",
        "\n",
        "\n",
        "$\\rightarrow$ I used the uniformly sampling method to collect the dataset, because we saw part 2, this method gave better results for the estimation of Q and V."
      ],
      "metadata": {
        "id": "CW1_-u3rjJQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Comparaison of the results with Value Iteration\n",
        "\n",
        "# Parameters\n",
        "tol = 1e-3\n",
        "gamma = 0.9\n",
        "\n",
        "VI_Q, VI_greedypol, all_qfunctions, VI_nb_it, VI_list_time_it = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
        "\n",
        "\n",
        "mae_FQI = mean_absolute_error(VI_Q, Q_fqi)\n",
        "print(\"MAE for FQI method :\",mae_FQI)\n",
        "mse_FQI = mean_squared_error(VI_Q, Q_fqi)\n",
        "print(\"MSE for FQI method :\",mse_FQI)\n"
      ],
      "metadata": {
        "id": "720uVa97ddWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "render_policy(env, VI_greedypol, horizon=100)\n",
        "img = env.get_layout_img(VI_Q.max(axis=1))    \n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A30iWFs7lAxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both policies lead the red diamond to its objectif, but we can see on the map that true V is higher than estimated V with FQI."
      ],
      "metadata": {
        "id": "CUjKgYQIk4PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Compute Q_fqi for diffenrent sigma and dim\n",
        "\n",
        "list_sigma = [0.1, 0.3, 0.6, 0.9]\n",
        "list_dim = [10,30,50, 80]\n",
        "\n",
        "list_param_1 =[]\n",
        "list_param_2 =[]\n",
        "list_Q_fqi = []\n",
        "for dim in list_dim :\n",
        "  for sigma in list_sigma:\n",
        "    print(\"feat map with sigma =\",sigma, 'and dim = ', dim)\n",
        "    list_param_1.append(dim)\n",
        "    list_param_2.append(sigma)\n",
        "    \n",
        "    feat_map = GridWorldFeatureMap(env, dim=50, sigma=0.25)\n",
        "\n",
        "    theta = linear_fqi(env, feat_map, num_iterations= 100)\n",
        "\n",
        "    # Compute and run greedy policy\n",
        "    Q_fqi = np.zeros((env.Ns, env.Na))\n",
        "    for ss in range(env.Ns):\n",
        "      state_feat = feat_map.map(ss)\n",
        "      Q_fqi[ss, :] = state_feat @ theta\n",
        "\n",
        "    list_Q_fqi.append(Q_fqi)"
      ],
      "metadata": {
        "id": "hDbWMAUBc6lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_mae = []\n",
        "for q in list_Q_fqi:\n",
        " list_mae.append(mean_absolute_error(VI_Q, q))"
      ],
      "metadata": {
        "id": "WUrmBviWdCpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(list(zip(list_param_1, list_param_2, list_mae)), columns =[\"Dim\",\"Sigma\",\"MAE\"])\n",
        "plt.scatter(df.Dim, df.Sigma, c=df.MAE)\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Dim\")\n",
        "plt.ylabel(\"Sigma\")\n",
        "plt.title(\"MAE according to dim and sigma of feat map\")"
      ],
      "metadata": {
        "id": "Cf4tx3vnhrVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the estimation of Q with FQI is better with bigger dimension of features map (>50) and with sigma around 0.5."
      ],
      "metadata": {
        "id": "g6vPJIqskqSj"
      }
    }
  ]
}